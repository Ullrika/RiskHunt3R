<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Ullrika Sahlin, Dawei Tang and Elias Zgheib" />

<meta name="date" content="2022-07-08" />

<title>RiskHunt3r workshop on uncertainty</title>

<script src="ProbMCdemo_files/header-attrs-2.13/header-attrs.js"></script>
<script src="ProbMCdemo_files/jquery-3.5.1/jquery-3.5.1.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="ProbMCdemo_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="ProbMCdemo_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="ProbMCdemo_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="ProbMCdemo_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="ProbMCdemo_files/navigation-1.1/tabsets.js"></script>
<link href="ProbMCdemo_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="ProbMCdemo_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">RiskHunt3r workshop on uncertainty</h1>
<h4 class="author">Ullrika Sahlin, Dawei Tang and Elias Zgheib</h4>
<h4 class="date">2022-07-08</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#load-and-plot-data-set">Load and plot data set</a></li>
<li><a href="#specify-a-bayesian-model">Specify a Bayesian model</a>
<ul>
<li><a href="#run-mcmc-sampling">Run MCMC sampling</a></li>
<li><a href="#study-summary-of-the-mcmc-sample">Study summary of the
MCMC sample</a></li>
<li><a
href="#study-the-convergence-of-the-samples-of-the-parameters">Study the
convergence of the samples of the parameters</a></li>
<li><a href="#compare-model-predictions-to-data">Compare model
predictions to data</a></li>
<li><a href="#uncertainty-about-the-model">Uncertainty about the
model</a></li>
<li><a href="#uncertainty-about-future-data-points">Uncertainty about
future data points</a></li>
<li><a
href="#an-alternative-way-to-illustrate-uncertainty-about-the-model-using-animated-plots">An
alternative way to illustrate uncertainty about the model using animated
plots</a></li>
</ul></li>
<li><a
href="#monte-carlo-simulation-to-generate-model-predictions-given-uncertainty-quantified-by-bayesian-inference">Monte
Carlo simulation to generate model predictions given uncertainty
quantified by Bayesian inference</a>
<ul>
<li><a
href="#illustrate-uncertainty-in-future-data-using-the-predictive-distribution">Illustrate
uncertainty in future data using the predictive distribution</a></li>
<li><a
href="#illustrate-uncertainty-in-future-value-using-the-2-dimensional-distribution">Illustrate
uncertainty in future value using the 2-dimensional
distribution</a></li>
</ul></li>
<li><a href="#specify-a-bayesian-model-with-a-squared-term">Specify a
Bayesian model with a squared term</a>
<ul>
<li><a href="#uncertainty-about-the-model-1">Uncertainty about the
model</a></li>
<li><a href="#uncertainty-about-the-future-data">Uncertainty about the
future data</a></li>
</ul></li>
<li><a href="#bayesian-model-selection-and-model-averaging">Bayesian
model selection and model averaging</a>
<ul>
<li><a href="#estimated-model-weights">Estimated model weights</a></li>
<li><a
href="#uncertainty-in-predictions-for-the-averaged-model">Uncertainty in
predictions for the averaged model</a></li>
<li><a
href="#uncertainty-about-the-average-model-using-animated-plots">Uncertainty
about the average model using animated plots</a></li>
</ul></li>
<li><a href="#comparison-to-interval-and-interval-analysis">Comparison
to interval and interval analysis</a>
<ul>
<li><a href="#interval-analysis-of-the-linear-model">Interval analysis
of the linear model</a></li>
<li><a
href="#interval-analysis-of-the-model-with-a-squared-term">Interval
analysis of the model with a squared term</a></li>
</ul></li>
</ul>
</div>

<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This is a demonstration of Quantitative uncertainty analysis in a
Bayesian framework.</p>
<p>Generate an artificial data set for a continuous response
variable.</p>
<p>Build and inform a linear model predicting the response given a
single covariate.</p>
<p>Demonstrate different ways to use the posterior distribution of the
parameters to evaluate the combined impact of parameter uncertainty on
an outcome or quantity of interest.</p>
<p>Model</p>
<p><span class="math display">\[Y = a + b \cdot X + e\]</span> <span
class="math display">\[ e \sim N(0,\sigma)\]</span> # Monte Carlo
simulation based on specified probability distributions</p>
<p>Let us start with a situation where we have uncertainty about
parameters</p>
<pre class="r"><code>n = 10^3 #number of iterations in the MC simulation 
a &lt;- rnorm(n,20,10)
b &lt;- rnorm(n,4,0.5)
sigma &lt;- abs(rnorm(n,3,10))
x &lt;- 60
y &lt;- unlist(lapply(1:n,function(i){a[i]+b[i]*(x-50)/50 + rnorm(1,0,sigma[i])}))

df_y &lt;- data.frame(y = y, gr = y&lt;25)

##plot prob density for Y
ggplot(df_y,aes(x = y,fill = stat(x &lt; 25))) +
 stat_halfeye() +
  #stat_dotsinterval() +
  geom_vline(xintercept = c(25), linetype = &quot;dashed&quot;) +
  scale_fill_manual(values = c(&quot;gray80&quot;, &quot;skyblue&quot;)) +
  ylab(&#39;pdf&#39;) +
  xlab(&#39;y(x=60)&#39;) +
  theme(legend.position=&#39;none&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="load-and-plot-data-set" class="section level1">
<h1>Load and plot data set</h1>
<pre class="r"><code>load(&quot;demodata.Rdata&quot;)
 
ggplot(df,aes(x=x,y=y))+
  geom_point()</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="specify-a-bayesian-model" class="section level1">
<h1>Specify a Bayesian model</h1>
<pre class="r"><code># create a list with data to go into the Bayesian sampler
data_jags &lt;- list(x=df$x, y=df$y, n=nrow(df))

mod_jags &lt;- function(){
    # Priors:
    a ~ dnorm(0, 0.001) # intercept
    b ~ dnorm(0, 0.001) # slope
    sigma ~ dunif(0, 100) # standard deviation
    tau &lt;- 1 / (sigma * sigma) # sigma^2 doesn&#39;t work in JAGS
    
    # Likelihood:
    for (i in 1:n){
        y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
        mu[i] &lt;- a + b * (x[i]-50)/50
    }
    
}</code></pre>
<div id="run-mcmc-sampling" class="section level2">
<h2>Run MCMC sampling</h2>
<pre class="r"><code># select initial values for the MCMC sampling
init_values &lt;- function(){
    list(a = rnorm(1), b = rnorm(1), sigma = abs(rnorm(1,10,2)))
}

# parameters to save 
params &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;sigma&quot;)

# run MCMC sampling using Gibbs sampling (may take some time, but not long)
mcmc_jags &lt;- jags(data = data_jags, inits = init_values, parameters.to.save = params, model.file = mod_jags,
               n.chains = 3, n.iter = 12000, n.burnin = 2000, n.thin = 10)</code></pre>
<pre><code>## module glm loaded</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 20
##    Unobserved stochastic nodes: 3
##    Total graph size: 131
## 
## Initializing model</code></pre>
<pre class="r"><code>mod_mcmc &lt;- as.mcmc(mcmc_jags)</code></pre>
</div>
<div id="study-summary-of-the-mcmc-sample" class="section level2">
<h2>Study summary of the MCMC sample</h2>
<pre class="r"><code>mcmc_jags</code></pre>
<pre><code>## Inference for Bugs model at &quot;C:/Users/ekol-usa/AppData/Local/Temp/Rtmp2tHHJp/model451c28bb19c3.txt&quot;, fit using jags,
##  3 chains, each with 12000 iterations (first 2000 discarded), n.thin = 10
##  n.sims = 3000 iterations saved
##          mu.vect sd.vect   2.5%    25%    50%    75%   97.5%  Rhat n.eff
## a         20.725   0.594 19.533 20.342 20.722 21.106  21.882 1.003  1400
## b          5.033   1.095  2.904  4.320  5.036  5.736   7.234 1.001  3000
## sigma      2.617   0.490  1.865  2.276  2.542  2.889   3.742 1.002  1200
## deviance  93.552   2.813 90.370 91.484 92.820 94.870 100.755 1.004   890
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.9 and DIC = 97.5
## DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
</div>
<div id="study-the-convergence-of-the-samples-of-the-parameters"
class="section level2">
<h2>Study the convergence of the samples of the parameters</h2>
<pre class="r"><code>#plot(mod_mcmc)</code></pre>
</div>
<div id="compare-model-predictions-to-data" class="section level2">
<h2>Compare model predictions to data</h2>
<p>Let us illustrate the model for values on x between 0 and 100.</p>
<pre class="r"><code># save posterior from three chains into one sample 
mcmc_sample &lt;- as.mcmc(rbind(mod_mcmc[[1]], mod_mcmc[[2]], mod_mcmc[[3]]))

# make predictions for different values on x 
x_new = seq(0,100,by=1)</code></pre>
</div>
<div id="uncertainty-about-the-model" class="section level2">
<h2>Uncertainty about the model</h2>
<pre class="r"><code>pred_sample2 &lt;- do.call(&#39;rbind&#39;,lapply(1:nrow(mcmc_sample),function(i){
data.frame(y=mcmc_sample[i,&quot;a&quot;] + (x_new-50)/50 * mcmc_sample[i,&quot;b&quot;],x=x_new,iter=i)
}))

ggplot(pred_sample2,aes(x=x,y=y)) +
    stat_lineribbon(aes(y = y), .width = c(.99, .95, .8, .5), color = &quot;#08519C&quot;) + 
  scale_fill_brewer() +
  geom_point(data=df,aes(x=x,y=y)) +
  ggtitle(&#39;Bayesian linear regression&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="uncertainty-about-future-data-points" class="section level2">
<h2>Uncertainty about future data points</h2>
<pre class="r"><code>pred_sample3 &lt;- do.call(&#39;rbind&#39;,lapply(1:nrow(mcmc_sample),function(i){
data.frame(y=mcmc_sample[i,&quot;a&quot;] + (x_new-50)/50 * mcmc_sample[i,&quot;b&quot;] + rnorm(length(x_new),0,mcmc_sample[i,&quot;sigma&quot;]),x=x_new,iter=i)
}))

ggplot(pred_sample3,aes(x=x,y=y)) +
    stat_lineribbon(aes(y = y), .width = c(.99, .95, .8, .5), color = &quot;#08519C&quot;) + 
  scale_fill_brewer() +
  geom_point(data=df,aes(x=x,y=y)) +
  ggtitle(&#39;Bayesian linear regression&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div
id="an-alternative-way-to-illustrate-uncertainty-about-the-model-using-animated-plots"
class="section level2">
<h2>An alternative way to illustrate uncertainty about the model using
animated plots</h2>
<pre class="r"><code>pred_sample_draws &lt;- do.call(&#39;rbind&#39;,lapply(1:20,function(j){
  i &lt;- sample.int(nrow(mcmc_sample),1)
data.frame(y=mcmc_sample[i,&quot;a&quot;] + (x_new-50)/50 * mcmc_sample[i,&quot;b&quot;],x=x_new,draw=j)
}))
 ggplot(pred_sample_draws,aes(x=x,y=y)) +
  geom_line(aes(group = draw), color = &quot;#08519C&quot;) +
  geom_point(data = df) +
  theme_bw() +
  transition_states(draw, 0, 0.2) +
  shadow_mark(past = TRUE, future = TRUE, alpha = 1/8, color = &quot;gray50&quot;)</code></pre>
<p><img
src="ProbMCdemo_files/figure-html/unnamed-chunk-10-1.gif" /><!-- --></p>
</div>
</div>
<div
id="monte-carlo-simulation-to-generate-model-predictions-given-uncertainty-quantified-by-bayesian-inference"
class="section level1">
<h1>Monte Carlo simulation to generate model predictions given
uncertainty quantified by Bayesian inference</h1>
<p>Monte Carlo simulation can be used to evaluate the combined impact of
parameter uncertainty on a prediction from the model.</p>
<p>Let us say we are interested in the value of y when x = 60.</p>
<p>Different ways to perform Monte Carlo simulation based on an MCMC
sample from the posterior distribution of a Bayesian model</p>
<ol style="list-style-type: decimal">
<li><p>summarise on the marginal distributions (and fit distributions to
it) and sample from these to run the MC simulation</p></li>
<li><p>plug in the MCMC sample into the MC simulation directly (useful
to consider dependencies between parameters)</p></li>
<li><p>generate the quantity of interest inside the MCMC
sampling</p></li>
</ol>
<div
id="illustrate-uncertainty-in-future-data-using-the-predictive-distribution"
class="section level2">
<h2>Illustrate uncertainty in future data using the predictive
distribution</h2>
<p>We want to characterise uncertainty in a potential future observation
of y, which includes how data may vary around the model and uncertainty
about the model.</p>
<pre class="r"><code>## derive quantity of interest from the posterior sample
a &lt;- mcmc_sample[,&quot;a&quot;]
b &lt;- mcmc_sample[,&quot;b&quot;]
sigma &lt;- mcmc_sample[,&quot;sigma&quot;]
x &lt;- 60
y_mc &lt;- unlist(lapply(1:nrow(mcmc_sample),function(i){
  a[i]+b[i]*(x-50)/50 + rnorm(1,0,sigma[i])}))

df_y &lt;- data.frame(y = y_mc, gr = y&lt;25)

##plot prob density for Y
ggplot(df_y,aes(x = y,fill = stat(x &lt; 25))) +
 stat_halfeye() +
  geom_vline(xintercept = c(25), linetype = &quot;dashed&quot;) +
  scale_fill_manual(values = c(&quot;gray80&quot;, &quot;skyblue&quot;)) +
  ylab(&#39;pdf&#39;) +
  xlab(&#39;y(x=60)&#39;) +
  theme(legend.position=&#39;none&#39;) +
  ggtitle(&#39;Uncertainty in future data&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div
id="illustrate-uncertainty-in-future-value-using-the-2-dimensional-distribution"
class="section level2">
<h2>Illustrate uncertainty in future value using the 2-dimensional
distribution</h2>
<p>Consider that we are interested in how variable y varies. If so, we
can illustrate uncertainty about this variability by propagating
uncertainty distinguishing variability from uncertainty. This is done
using 2-dimenstional Monte Carlo simulation. The resulting graph is
describing a 2-D distribution or a spaghetti plot.</p>
<pre class="r"><code>## derive quantity of interest from the posterior sample
y_sample_draws &lt;- do.call(&#39;rbind&#39;,lapply(1:20,function(j){
  i &lt;- sample.int(nrow(mcmc_sample),1)
  x &lt;- 60
  pp &lt;- ppoints(100)
  qq &lt;- qnorm(pp,mcmc_sample[i,&quot;a&quot;] + (x-50)/50 * mcmc_sample[i,&quot;b&quot;],mcmc_sample[i,&quot;sigma&quot;])
  
data.frame(pdf=dnorm(qq,mcmc_sample[i,&quot;a&quot;] + (x-50)/50 * mcmc_sample[i,&quot;b&quot;],mcmc_sample[i,&quot;sigma&quot;]),
           q=qq,draw=j)
}))

ggplot(y_sample_draws,aes(x = q,y = pdf)) +
  geom_line(aes(group = draw), color = &quot;#08519C&quot;) +
  geom_vline(xintercept = c(25), linetype = &quot;dashed&quot;) +
  ylab(&#39;pdf&#39;) +
  xlab(&#39;y(x=60)&#39;) +
  theme(legend.position=&#39;none&#39;) +
  theme_bw() +
  transition_states(draw, 0, 0.2) +
  shadow_mark(past = TRUE, future = TRUE, alpha = 1/8, color = &quot;gray50&quot;)+
  ggtitle(&#39;Uncertainty in a future value distinguishing variability from uncertainty&#39;)</code></pre>
<p><img
src="ProbMCdemo_files/figure-html/unnamed-chunk-12-1.gif" /><!-- --></p>
</div>
</div>
<div id="specify-a-bayesian-model-with-a-squared-term"
class="section level1">
<h1>Specify a Bayesian model with a squared term</h1>
<p>Model with squared term <span class="math display">\[Y = a + b \cdot
X + c \cdot X^2 + e\]</span></p>
<p><span class="math display">\[ e \sim N(0,\sigma)\]</span></p>
<pre class="r"><code>mod_jags_sq &lt;- function(){
    # Priors:
    a ~ dnorm(0, 0.001) # intercept
    b ~ dnorm(0, 0.001) # linear term
    c ~ dnorm(0, 0.001) # squared term
    sigma ~ dunif(0, 100) # standard deviation
    tau &lt;- 1 / (sigma * sigma) # sigma^2 doesn&#39;t work in JAGS
    
    # Likelihood:
    for (i in 1:n){
        y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
        mu[i] &lt;- a + b * (x[i]-50)/50 + c * (x[i]-50)/50 * (x[i]-50)/50
    }
    
}

# select initial values for the MCMC sampling
init_values_sq &lt;- function(){
    list(a = rnorm(1), b = rnorm(1), c = rnorm(1), sigma = abs(rnorm(1,10,2)))
}

# parameters to save 
params_sq &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;sigma&quot;)

# run MCMC sampling using Gibbs sampling (may take some time, but not long)
mcmc_jags_sq &lt;- jags(data = data_jags, inits = init_values_sq, parameters.to.save = params_sq, model.file = mod_jags_sq,
               n.chains = 3, n.iter = 12000, n.burnin = 2000, n.thin = 10)</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 20
##    Unobserved stochastic nodes: 4
##    Total graph size: 212
## 
## Initializing model</code></pre>
<pre class="r"><code>mod_mcmc_sq &lt;- as.mcmc(mcmc_jags_sq)

# study summary of output
mcmc_jags_sq</code></pre>
<pre><code>## Inference for Bugs model at &quot;C:/Users/ekol-usa/AppData/Local/Temp/Rtmp2tHHJp/model451c708642d3.txt&quot;, fit using jags,
##  3 chains, each with 12000 iterations (first 2000 discarded), n.thin = 10
##  n.sims = 3000 iterations saved
##          mu.vect sd.vect   2.5%    25%    50%    75%   97.5%  Rhat n.eff
## a         21.493   0.814 19.896 20.976 21.508 22.039  23.056 1.001  3000
## b          4.712   1.109  2.450  3.992  4.708  5.450   6.866 1.001  3000
## c         -2.612   2.019 -6.778 -3.866 -2.606 -1.305   1.366 1.001  3000
## sigma      2.564   0.473  1.823  2.228  2.522  2.828   3.740 1.001  3000
## deviance  92.510   3.282 88.414 90.115 91.856 94.108 100.896 1.002  2000
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 5.4 and DIC = 97.9
## DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<pre class="r"><code># Study the convergence of the samples of the parameters 
#plot(mod_mcmc_sq)</code></pre>
<div id="uncertainty-about-the-model-1" class="section level2">
<h2>Uncertainty about the model</h2>
<pre class="r"><code># save posterior from three chains into one sample 
mcmc_sample_sq &lt;- as.mcmc(rbind(mod_mcmc_sq[[1]], mod_mcmc_sq[[2]], mod_mcmc_sq[[3]]))

# make predictions for different values on x 
x_new = seq(0,100,by=1)

pred_sample2_sq &lt;- do.call(&#39;rbind&#39;,lapply(1:nrow(mcmc_sample_sq),function(i){
data.frame(y=mcmc_sample_sq[i,&quot;a&quot;] + (x_new-50)/50 * mcmc_sample_sq[i,&quot;b&quot;] + (x_new-50)/50 * (x_new-50)/50 * mcmc_sample_sq[i,&quot;c&quot;],x=x_new,iter=i)
}))

ggplot(pred_sample2_sq,aes(x=x,y=y)) +
    stat_lineribbon(aes(y = y), .width = c(.99, .95, .8, .5), color = &quot;#08519C&quot;) + 
  scale_fill_brewer() +
  geom_point(data=df,aes(x=x,y=y)) +
  ggtitle(&#39;Bayesian regression with squared term&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="uncertainty-about-the-future-data" class="section level2">
<h2>Uncertainty about the future data</h2>
<pre class="r"><code>pred_sample3_sq &lt;- do.call(&#39;rbind&#39;,lapply(1:nrow(mcmc_sample_sq),function(i){
data.frame(y=mcmc_sample_sq[i,&quot;a&quot;] + (x_new-50)/50 * mcmc_sample_sq[i,&quot;b&quot;] + (x_new-50)/50 * (x_new-50)/50 * mcmc_sample_sq[i,&quot;c&quot;] + rnorm(length(x_new),0,mcmc_sample_sq[i,&quot;sigma&quot;]),x=x_new,iter=i)
}))

ggplot(pred_sample3_sq,aes(x=x,y=y)) +
    stat_lineribbon(aes(y = y), .width = c(.99, .95, .8, .5), color = &quot;#08519C&quot;) + 
  scale_fill_brewer() +
  geom_point(data=df,aes(x=x,y=y)) +
  ggtitle(&#39;Bayesian regression with squared term&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
</div>
<div id="bayesian-model-selection-and-model-averaging"
class="section level1">
<h1>Bayesian model selection and model averaging</h1>
<p>There are plenty of nice ways to compare alternative Bayesian models.
Here we use the DIC-values (an estimate of the prediction error
considering uncertainty in parameters) to weight the two Bayesian models
when doing the MC-simulation.</p>
<div id="estimated-model-weights" class="section level2">
<h2>Estimated model weights</h2>
<pre class="r"><code>model_dics &lt;- c(mcmc_jags$BUGSoutput$DIC, mcmc_jags_sq$BUGSoutput$DIC)
w &lt;- exp(-model_dics/2)
df_w &lt;- data.frame(weight = w/sum(w), model = c(&#39;linear&#39;,&#39;squared&#39;))
ggplot(df_w,aes(x=model,y=weight,fill = model)) +
  geom_bar(stat=&#39;identity&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="uncertainty-in-predictions-for-the-averaged-model"
class="section level2">
<h2>Uncertainty in predictions for the averaged model</h2>
<pre class="r"><code>niter &lt;- min(nrow(mcmc_sample),nrow(mcmc_sample_sq))
iter &lt;- sample.int(2,niter,prob=df_w$weight,replace = TRUE)
pred_sample_ma &lt;- do.call(&#39;rbind&#39;,lapply(1:niter,function(i){

if(iter[i]==1){
y=mcmc_sample[i,&quot;a&quot;] + (x_new-50)/50 * mcmc_sample[i,&quot;b&quot;]
}else{
y=mcmc_sample_sq[i,&quot;a&quot;] + (x_new-50)/50 * mcmc_sample_sq[i,&quot;b&quot;] + (x_new-50)/50 * (x_new-50)/50 * mcmc_sample_sq[i,&quot;c&quot;]
}
data.frame(y=y,x=x_new,iter=i)
}
))

ggplot(pred_sample_ma,aes(x=x,y=y)) +
    stat_lineribbon(aes(y = y), .width = c(.99, .95, .8, .5), color = &quot;#08519C&quot;) + 
  scale_fill_brewer() +
  geom_point(data=df,aes(x=x,y=y)) +
  ggtitle(&#39;Bayesian model averaging&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="uncertainty-about-the-average-model-using-animated-plots"
class="section level2">
<h2>Uncertainty about the average model using animated plots</h2>
<pre class="r"><code>pred_sample_draws_av &lt;- do.call(&#39;rbind&#39;,lapply(1:20,function(j){
  i &lt;- sample.int(nrow(mcmc_sample),1)
  
  data.frame(pred_sample_ma[pred_sample_ma$iter == i,],draw = j)
}
))

 ggplot(pred_sample_draws_av,aes(x=x,y=y)) +
  geom_line(aes(group = draw), color = &quot;#08519C&quot;) +
  geom_point(data = df) +
  theme_bw() +
  transition_states(draw, 0, 0.2) +
  shadow_mark(past = TRUE, future = TRUE, alpha = 1/8, color = &quot;gray50&quot;)</code></pre>
<p><img
src="ProbMCdemo_files/figure-html/unnamed-chunk-18-1.gif" /><!-- --></p>
</div>
</div>
<div id="comparison-to-interval-and-interval-analysis"
class="section level1">
<h1>Comparison to interval and interval analysis</h1>
<p>Intervals determined by the 1st and 99th percentiles for uncertainty
in parameters and 1st and 99th percentiles for the variability.</p>
<p>There is a high agreement between output from interval analysis and
probabilistic analysis, because the intervals are informed by the
probabilistic analysis and the model is relatively simple.</p>
<div id="interval-analysis-of-the-linear-model" class="section level2">
<h2>Interval analysis of the linear model</h2>
<pre class="r"><code>a &lt;- quantile(mcmc_sample[,&quot;a&quot;],probs = c(0.025,0.975))
b &lt;- quantile(mcmc_sample[,&quot;b&quot;],probs = c(0.025,0.975))
sigma &lt;- quantile(mcmc_sample[,&quot;sigma&quot;],probs = c(0.025,0.975))
x &lt;- 60
y &lt;- c(min(a)+min(b*(x-50)/50) + qnorm(0.05,0,max(sigma)),
           max(a)+max(b*(x-50)/50) + qnorm(0.95,0,max(sigma)))

df_int &lt;- data.frame(quantity = c(&#39;a&#39;,&#39;b&#39;,&#39;sigma&#39;,&#39;y&#39;), lower = c(a[1],b[1],sigma[1],y[1]), upper = c(a[2], b[2], sigma[2], y[2]))

df_inputoutput &lt;- data.frame(sample = c(mcmc_sample[,&#39;a&#39;],mcmc_sample[,&#39;b&#39;],mcmc_sample[,&#39;sigma&#39;],y_mc),quantity = rep(c(&#39;a&#39;,&#39;b&#39;,&#39;sigma&#39;,&#39;y&#39;),each = nrow(mcmc_sample)))
                             
ggplot(df_inputoutput,aes(x = sample)) +
 stat_halfeye() +
  geom_segment(data = df_int, aes(x = lower, xend = upper, y = -0.1, yend = -0.1), col = &#39;red&#39;) + 
  facet_wrap(~quantity, scales = &quot;free&quot;) +
  ylab(&#39;pdf&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="interval-analysis-of-the-model-with-a-squared-term"
class="section level2">
<h2>Interval analysis of the model with a squared term</h2>
<pre class="r"><code>a &lt;- quantile(mcmc_sample_sq[,&quot;a&quot;],probs = c(0.025,0.975))
b &lt;- quantile(mcmc_sample_sq[,&quot;b&quot;],probs = c(0.025,0.975))
c &lt;- quantile(mcmc_sample_sq[,&quot;c&quot;],probs = c(0.025,0.975))
sigma &lt;- quantile(mcmc_sample[,&quot;sigma&quot;],probs = c(0.05,0.95))
x &lt;- 60
y &lt;- c(min(a)+min(b*(x-50)/50) +min(c*(x-50)/50 *(x-50)/50) + qnorm(0.05,0,max(sigma)),
           max(a)+max(b*(x-50)/50) +max(c*(x-50)/50 *(x-50)/50) + qnorm(0.95,0,max(sigma)))

df_int &lt;- data.frame(quantity = c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;sigma&#39;,&#39;y&#39;), lower = c(a[1],b[1],c[1],sigma[1],y[1]), upper = c(a[2], b[2], c[2], sigma[2], y[2]))

df_inputoutput &lt;- data.frame(sample = c(mcmc_sample_sq[,&#39;a&#39;],mcmc_sample_sq[,&#39;b&#39;],mcmc_sample_sq[,&#39;c&#39;],mcmc_sample_sq[,&#39;sigma&#39;],y_mc),quantity = rep(c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;sigma&#39;,&#39;y&#39;),each = nrow(mcmc_sample_sq)))
                             
ggplot(df_inputoutput,aes(x = sample)) +
 stat_halfeye() +
  geom_segment(data = df_int, aes(x = lower, xend = upper, y = -0.1, yend = -0.1), col = &#39;red&#39;) + 
  facet_wrap(~quantity, scales = &quot;free&quot;) +
  ylab(&#39;pdf&#39;)</code></pre>
<p><img src="ProbMCdemo_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
